{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nimesha\\AppData\\Local\\Temp\\ipykernel_12572\\3726734014.py:7: DeprecationWarning: Please use `median_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
      "  from scipy.ndimage.filters import median_filter\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import pydicom\n",
    "from scipy.ndimage.filters import median_filter\n",
    "from lungmask import mask\n",
    "import SimpleITK as sitk\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from easyfsl.samplers import TaskSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_hu(medical_image, image):\n",
    "    intercept = medical_image.RescaleIntercept\n",
    "    slope = medical_image.RescaleSlope\n",
    "    hu_image = image * slope + intercept\n",
    "\n",
    "    return hu_image\n",
    "\n",
    "def preprocess_images(img,dicom_image):\n",
    "\n",
    "    hu_image = transform_to_hu(dicom_image, img)\n",
    "\n",
    "    # medianl filter for noise reduction \n",
    "    # Apply the median filter with a kernel size of 3x3\n",
    "    filtered_image = median_filter(hu_image, size=(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DICOMDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "\n",
    "        self.img_labels = []\n",
    "        self.root_dir = root_dir\n",
    "        self.dcm_files = os.listdir(root_dir)\n",
    "\n",
    "        for filename in os.listdir(root_dir):\n",
    "            \n",
    "            image_name = filename\n",
    "            category = image_name[0]\n",
    "\n",
    "            if category =='A' :\n",
    "               label=1\n",
    "            elif category =='B':\n",
    "               label=2\n",
    "            elif category =='G':  \n",
    "               label=3 \n",
    "            elif category =='E':\n",
    "               label=4 \n",
    "            else : label=5        \n",
    "            # print(label)\n",
    "            self.img_labels.append((image_name,label))\n",
    "\n",
    "    def __len__(self):\n",
    "        # print(len(self.img_labels))\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        dcm_file = self.dcm_files[idx]\n",
    "        # print(dcm_file)\n",
    "        label_ch =dcm_file[0]\n",
    "        if label_ch =='A' :\n",
    "            label=1\n",
    "        elif label_ch =='B':\n",
    "            label=2\n",
    "        elif label_ch =='G':  \n",
    "            label=3 \n",
    "        elif label_ch =='E':\n",
    "            label=4  \n",
    "        else : label=5\n",
    "        dcm_path = os.path.join(self.root_dir, dcm_file)\n",
    "        \n",
    "        dicom_image= pydicom.dcmread(dcm_path)\n",
    "    \n",
    "        image = np.array(dicom_image.pixel_array)\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "       \n",
    "        image = image.astype('float32')\n",
    "        image = np.expand_dims(image, axis=0)  # Add a channel to the image\n",
    "        \n",
    "        \n",
    "        \n",
    "        image = torch.from_numpy(image)\n",
    "      \n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "from easyfsl.datasets import FewShotDataset\n",
    "import random\n",
    "from typing import Dict, Iterator, List, Tuple, Union\n",
    "from torch import Tensor\n",
    "\n",
    "GENERIC_TYPING_ERROR_MESSAGE = (\n",
    "    \"Check out the output's type of your dataset's __getitem__() method.\"\n",
    "    \"It must be a Tuple[Tensor, int] or Tuple[Tensor, 0-dim Tensor].\"\n",
    ")\n",
    "\n",
    "class TaskSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Samples batches in the shape of few-shot classification tasks. At each iteration, it will sample\n",
    "    n_way classes, and then sample support and query images from these classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: FewShotDataset,\n",
    "        n_way: int,\n",
    "        n_shot: int,\n",
    "        n_query: int,\n",
    "        n_tasks: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: dataset from which to sample classification tasks. Must have implement get_labels() from\n",
    "                FewShotDataset.\n",
    "            n_way: number of classes in one task\n",
    "            n_shot: number of support images for each class in one task\n",
    "            n_query: number of query images for each class in one task\n",
    "            n_tasks: number of tasks to sample\n",
    "        \"\"\"\n",
    "        super().__init__(data_source=None)\n",
    "        self.n_way = n_way\n",
    "        self.n_shot = n_shot\n",
    "        self.n_query = n_query\n",
    "        self.n_tasks = n_tasks\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        self.items_per_label: Dict[int, List[int]] = {}\n",
    "        for item, label in enumerate(dataset.get_labels()):\n",
    "            # print(item,label)\n",
    "            if label in self.items_per_label:\n",
    "                self.items_per_label[label].append(item)\n",
    "            else:\n",
    "                self.items_per_label[label] = [item]\n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "        self._check_dataset_size_fits_sampler_parameters()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.n_tasks\n",
    "    \n",
    "    def _check_dataset_size_fits_sampler_parameters(self):\n",
    "        \"\"\"\n",
    "        Check that the dataset size is compatible with the sampler parameters\n",
    "        \"\"\"\n",
    "        self._check_dataset_has_enough_labels()\n",
    "        self._check_dataset_has_enough_items_per_label()\n",
    "\n",
    "    def _check_dataset_has_enough_labels(self):\n",
    "        if self.n_way > len(self.items_per_label):\n",
    "            raise ValueError(\n",
    "                f\"The number of labels in the dataset ({len(self.items_per_label)} \"\n",
    "                f\"must be greater or equal to n_way ({self.n_way}).\"\n",
    "            )\n",
    "\n",
    "    def _check_dataset_has_enough_items_per_label(self):\n",
    "        number_of_samples_per_label = [\n",
    "            len(items_for_label) for items_for_label in self.items_per_label.values()\n",
    "        ]\n",
    "        minimum_number_of_samples_per_label = min(number_of_samples_per_label)\n",
    "        label_with_minimum_number_of_samples = number_of_samples_per_label.index(\n",
    "            minimum_number_of_samples_per_label\n",
    "        )\n",
    "        if self.n_shot + self.n_query > minimum_number_of_samples_per_label:\n",
    "            raise ValueError(\n",
    "                f\"Label {label_with_minimum_number_of_samples} has only {minimum_number_of_samples_per_label} samples\"\n",
    "                f\"but all classes must have at least n_shot + n_query ({self.n_shot + self.n_query}) samples.\"\n",
    "            )\n",
    "\n",
    "\n",
    "    def __iter__(self) -> Iterator[List[int]]:\n",
    "       \n",
    "        for i in range(self.n_tasks):\n",
    "         \n",
    "            yield torch.cat(\n",
    "                [\n",
    "                    # pylint: disable=not-callable\n",
    "                    torch.tensor(\n",
    "                        random.sample(\n",
    "                            self.items_per_label[label], self.n_shot + self.n_query\n",
    "                        )\n",
    "                    )\n",
    "                    # pylint: enable=not-callable\n",
    "                    for label in random.sample(sorted(self.items_per_label.keys()), self.n_way)\n",
    "                ]\n",
    "            ).tolist()\n",
    "    \n",
    "    def episodic_collate_fn(  self,input_data: List[Tuple[Tensor, Union[Tensor, int]]]\n",
    "    ) -> Tuple[Tensor, Tensor, Tensor, Tensor, List[int]]:\n",
    "        \n",
    "       \n",
    "        input_data_with_int_labels = self._cast_input_data_to_tensor_int_tuple(input_data)\n",
    "        true_class_ids = list({x[1] for x in input_data_with_int_labels})\n",
    "        \n",
    "        all_images = torch.cat([x[0].unsqueeze(0) for x in input_data_with_int_labels])\n",
    "        all_images = all_images.reshape(\n",
    "            (self.n_way, self.n_shot + self.n_query, *all_images.shape[1:])\n",
    "        )\n",
    "\n",
    "        # pylint: disable=not-callable\n",
    "        all_labels = torch.tensor(\n",
    "            [true_class_ids.index(x[1]) for x in input_data_with_int_labels]\n",
    "        ).reshape((self.n_way, self.n_shot + self.n_query))\n",
    "        # pylint: enable=not-callable\n",
    "        support_images = all_images[:, : self.n_shot].reshape(\n",
    "            (-1, *all_images.shape[2:])\n",
    "        )\n",
    "\n",
    "\n",
    "        query_images = all_images[:, self.n_shot :].reshape((-1, *all_images.shape[2:]))\n",
    "        support_labels = all_labels[:, : self.n_shot].flatten()\n",
    "        query_labels = all_labels[:, self.n_shot :].flatten()\n",
    "        return (\n",
    "            support_images,\n",
    "            support_labels,\n",
    "            query_images,\n",
    "            query_labels,\n",
    "            true_class_ids,\n",
    "        )\n",
    "        \n",
    "\n",
    "    \n",
    "    def _cast_input_data_to_tensor_int_tuple(\n",
    "                           self,input_data: List[Tuple[Tensor, Union[Tensor, int]]]\n",
    "    ) -> List[Tuple[Tensor, int]]:\n",
    "        \"\"\"\n",
    "        Check the type of the input for the episodic_collate_fn method, and cast it to the right type if possible.\n",
    "        Args:\n",
    "            input_data: each element is a tuple containing:\n",
    "                - an image as a torch Tensor\n",
    "                - the label of this image as an int or a 0-dim tensor\n",
    "        Raises:\n",
    "            TypeError : Wrong type of input images or labels\n",
    "            ValueError: Input label is not a 0-dim tensor\n",
    "        \"\"\"\n",
    "        for image, label in input_data:\n",
    "            \n",
    "            if not isinstance(image, Tensor):\n",
    "                raise TypeError(\n",
    "                    f\"Illegal type of input instance: {type(image)}. \"\n",
    "                    + GENERIC_TYPING_ERROR_MESSAGE\n",
    "                )\n",
    "            if not isinstance(label, int):\n",
    "                if not isinstance(label, Tensor):\n",
    "                    raise TypeError(\n",
    "                        f\"Illegal type of input label: {type(label)}. \"\n",
    "                        + \"GENERIC_TYPING_ERROR_MESSAGE\"\n",
    "                    )\n",
    "                if label.dtype not in {\n",
    "                    torch.uint8,\n",
    "                    torch.int8,\n",
    "                    torch.int16,\n",
    "                    torch.int32,\n",
    "                    torch.int64,\n",
    "                }:\n",
    "                    raise TypeError(\n",
    "                        f\"Illegal dtype of input label tensor: {label.dtype}. \"\n",
    "                        + \"GENERIC_TYPING_ERROR_MESSAGE\"\n",
    "                    )\n",
    "                if label.ndim != 0:\n",
    "                    raise ValueError(\n",
    "                        f\"Illegal shape for input label tensor: {label.shape}. \"\n",
    "                        + \"GENERIC_TYPING_ERROR_MESSAGE\"\n",
    "                    )\n",
    "\n",
    "        return [(image, int(label)) for (image, label) in input_data]\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = DICOMDataset(root_dir='../IMAGES/DICOM_SUPPORT/', transform=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sampler needs a dataset with a \"get_labels\" method. Check the code if you have any doubt!\n",
    "dataset.get_labels = lambda: [\n",
    "    instance[1] for instance in dataset.img_labels\n",
    "]\n",
    "\n",
    "test_sampler = TaskSampler(\n",
    "    dataset, n_way=4, n_shot=2, n_query=2, n_tasks=2\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_sampler=test_sampler,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    collate_fn=test_sampler.episodic_collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "(\n",
    "    example_support_images,\n",
    "    example_support_labels,\n",
    "    example_query_images,\n",
    "    example_query_labels,\n",
    "    example_class_ids,\n",
    ") = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(example_support_images))\n",
    "print(len(example_query_images))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
